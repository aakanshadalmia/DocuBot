{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2416b63-f1bb-4f1d-9aad-6910260dac99",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "*DocuBot is a specialized chatbot designed for efficiently retrieving case-specific information from a collection of documents. By leveraging a pre-trained **large language model**, users can query DocuBot using natural language and receive relevant information from the document set.*\n",
    "\n",
    "*Behind the scenes, DocuBot generates **word embeddings** for input documents, organizes and stores them in a **PostgreSQL database**. When a user submits a query, DocuBot searches the database to contextually relevant responses.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ea259-e743-4d64-9348-dd683739905f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03d7b1c-70bf-4bfb-987e-d09ef1f231bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pgvector\n",
    "import vertexai\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from psycopg2 import pool\n",
    "from loguru import logger\n",
    "from itertools import chain\n",
    "from pydantic import BaseModel\n",
    "from psycopg2.extras import execute_values\n",
    "from llama_index.core.schema import Document\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from vertexai.language_models import ChatModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from llama_index.core.text_splitter import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e71f3-f3c5-48d1-bfd1-e7cbe42d771b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Document Ingestion\n",
    "   \n",
    "- Preprocesses raw input, splitting it into logical segments.\n",
    "- Generates text embeddings for each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0716e2fc-7785-4417-894c-96b045fa5eb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Split Text to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88a6b79c-a1e0-4da8-a5b0-2fb3d4aecbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_to_chunks(input_text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a sentence into chunks\n",
    "    Input:\n",
    "        text : Text to be split\n",
    "    Output:\n",
    "        chunks: Segments of text after splitting\n",
    "    \"\"\"\n",
    "\n",
    "    # Parsing text with a preference for complete sentences\n",
    "    text_splitter = SentenceSplitter(\n",
    "        separator = \" \",\n",
    "        chunk_size = 300,\n",
    "        chunk_overlap = 20,\n",
    "        paragraph_separator = \"\\n\\n\",\n",
    "        secondary_chunking_regex = \"[^,.;。]+[,.;。]?\",\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode,\n",
    "    )\n",
    "    \n",
    "    txt_doc = Document(text = input_text)    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter([txt_doc])\n",
    "\n",
    "    return [chunk.text for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c0afd-4917-4a3b-a117-3ad5d70d1573",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 Generate Text Embeddings\n",
    "\n",
    "\\- Using a pre-trained model by Vertex AI to generate embeddings for input chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032e4389-5cbd-4feb-a28c-d43236df7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(BaseModel):\n",
    "    text : str\n",
    "    embedding : list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dc95ae9-cc36-4316-82c6-da03229c6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text) -> list[float]:    \n",
    "    \"\"\"\n",
    "    Generate embeddings for given text\n",
    "    Input:\n",
    "        text : Input text   \n",
    "    Output:\n",
    "        vector: Emdedding of the input text\n",
    "    \"\"\"\n",
    "    \n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    embeddings = model.get_embeddings([text])\n",
    "    \n",
    "    for embedding in embeddings:\n",
    "        vector = embedding.values             \n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9941c1-cfe4-4c70-8f0a-aedc3703e9f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.3 Generate text, embedding pair for all chunks in a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cb64241-46ce-403a-882a-896370f0cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding_pairs(text : str) -> list[ChunkEmbedding]:\n",
    "    \"\"\"\n",
    "    Get all the chunks and corresponding embeddings for a given text\n",
    "    Input:\n",
    "        text : Text whose chunk and embedding is needed\n",
    "    Output:\n",
    "        chunk_embedding_pairs: chunk and embedding of given text\n",
    "    \"\"\"    \n",
    "    \n",
    "    chunks : list[str] = split_input_to_chunks(text) \n",
    "    chunk_embedding_pairs : list[TextEmbedding] = []\n",
    "    logger.info(f'Number of chunks generated: {len(chunks)}')\n",
    "    \n",
    "    for curr_chunk in chunks:        \n",
    "        curr_embedding = text_embedding(curr_chunk)\n",
    "        chunk_embedding_pairs.append(TextEmbedding(text = curr_chunk, embedding = curr_embedding))    \n",
    "    \n",
    "    return chunk_embedding_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c8f04-3329-46a4-a6c9-fe43b0f2937c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Setting up the Cloud Database\n",
    "\n",
    "- Implementing functions to create and manage a database table for storing text chunks and their embeddings.\n",
    "- Utilizing connection pooling to efficiently manage database connections and reduce overhead.\n",
    "- Providing methods for inserting data into the table (ingest) and retrieving data from the table (retrieve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c4cea-4935-470e-9561-6c7cfdaf6be6",
   "metadata": {},
   "source": [
    "**Initializing Vertex AI environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48438211-986a-46c5-bd60-586348643718",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project = \"inductive-world-416413\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419283e3-6efb-429c-8a20-588c613a93a0",
   "metadata": {},
   "source": [
    "**Initializing DB parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f3300dc-d909-498b-ab33-f2276f27e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PARAMS = {\n",
    "    'dbname' : \"vectordb\",\n",
    "    'user' : \"user\",\n",
    "    'password' : \"pwd\",\n",
    "    'host' : \"localhost\",\n",
    "    'port' : \"5432\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118353f-c38c-49c3-aa26-9818c58af6e1",
   "metadata": {},
   "source": [
    "**Setting up the Datastore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "422b765d-b3f7-45ce-b4ad-0121e6e64f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStore:\n",
    "    \n",
    "    DATABASE_SCHEMA = {\n",
    "        \"text_chunk\" : \"varchar\",\n",
    "        \"embedding\" : \"vector(768)\"\n",
    "    }\n",
    "    \n",
    "    TABLE_NAME = \"my_table\"\n",
    "    \n",
    "    def __init__(self, db_params : dict = DB_PARAMS):\n",
    "        self.db_params = db_params        \n",
    "        self.conn_pool = self._get_connection_pool()        \n",
    "        self._create_table()\n",
    "    \n",
    "    def _get_connection_pool(self):\n",
    "        return psycopg2.pool.SimpleConnectionPool(1, 10, **self.db_params)\n",
    "\n",
    "    def _create_table(self) -> None:\n",
    "        col_defs = [f'{col_name} {col_type}' for col_name, col_type in self.DATABASE_SCHEMA.items()]        \n",
    "        cols = \", \".join(col_defs)        \n",
    "        table_creation_query = f\"\"\"\n",
    "            CREATE EXTENSION IF NOT EXISTS vector;\n",
    "            DROP TABLE IF EXISTS {self.TABLE_NAME};\n",
    "            CREATE TABLE IF NOT EXISTS {self.TABLE_NAME} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            {cols}\n",
    "            );\n",
    "            \"\"\"   \n",
    "        logger.info(table_creation_query)\n",
    "        try:\n",
    "            connection = self.conn_pool.getconn()\n",
    "            with connection:\n",
    "                with connection.cursor() as cursor:\n",
    "                    cursor.execute(table_creation_query)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in create table query: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.conn_pool.putconn(connection)\n",
    "    \n",
    "    def ingest(self, text: str) -> None:\n",
    "        text_embedding_pairs : list[TextChunk] = get_text_embedding_pairs(text)\n",
    "        data_list = [(curr.text, curr.embedding) for curr in text_embedding_pairs]\n",
    "        print(data_list[0][0])\n",
    "        col_names = \",\".join(list(self.DATABASE_SCHEMA.keys()))\n",
    "        table_update_query = f\"\"\"\n",
    "            INSERT INTO {self.TABLE_NAME} \n",
    "            ( {col_names} )\n",
    "            VALUES %s\n",
    "            \"\"\"                    \n",
    "        try:            \n",
    "            connection = self.conn_pool.getconn()\n",
    "            with connection:\n",
    "                with connection.cursor() as cursor:\n",
    "                    execute_values(cursor, table_update_query, data_list)\n",
    "                    logger.info(\"Updated table with embedding pairs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in update table query : {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.conn_pool.putconn(connection)\n",
    "\n",
    "    def retrieve(self, query: str) -> str:\n",
    "        query_embedding : list[float] = text_embedding(query)\n",
    "        retrieval_query = f\"\"\"\n",
    "            SELECT text_chunk FROM {self.TABLE_NAME}\n",
    "            ORDER BY embedding <-> %s LIMIT 1\n",
    "            \"\"\"\n",
    "        retrieved_chunk = \"\"\n",
    "        try:            \n",
    "            connection = self.conn_pool.getconn()\n",
    "            register_vector(connection)\n",
    "            with connection:\n",
    "                with connection.cursor() as cursor:\n",
    "                    cursor.execute(retrieval_query, (np.array(query_embedding, dtype = np.float64), ))\n",
    "                    retrieved_chunk = list(chain.from_iterable(cursor.fetchall()))\n",
    "                    logger.info(f\"Retreived {len(retrieved_chunk)} chunk for the given embedding\")            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in retrieval query : {e}\")\n",
    "            raise  \n",
    "        finally:\n",
    "            self.conn_pool.putconn(connection)\n",
    "        return retrieved_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfdd686-803f-4823-9550-838fee06bc20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Ingest Documents to Database\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1efeca40-1250-4a0f-83fc-593b61f287c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:25:26.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_create_table\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m\n",
      "            CREATE EXTENSION IF NOT EXISTS vector;\n",
      "            DROP TABLE IF EXISTS my_table;\n",
      "            CREATE TABLE IF NOT EXISTS my_table (\n",
      "            id SERIAL PRIMARY KEY,\n",
      "            text_chunk varchar, embedding vector(768)\n",
      "            );\n",
      "            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datastore = DataStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3297a0a3-747e-42ba-9273-5a44af26a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"John lives in America. John has two kids. \" * 100\n",
    "input2 = \"I am Aakansha and I live in India. \" * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29314f41-c8d7-481f-b0f2-1eef25831a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:25:27.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_text_embedding_pairs\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mNumber of chunks generated: 5\u001b[0m\n",
      "\u001b[32m2024-03-15 02:25:46.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mUpdated table with embedding pairs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids. John lives in America. John has two kids.\n"
     ]
    }
   ],
   "source": [
    "datastore.ingest(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "122e46e7-c6a0-49e7-ae7b-57e6498f61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:25:46.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_text_embedding_pairs\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mNumber of chunks generated: 5\u001b[0m\n",
      "\u001b[32m2024-03-15 02:26:05.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mUpdated table with embedding pairs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India. I am Aakansha and I live in India.\n"
     ]
    }
   ],
   "source": [
    "datastore.ingest(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc986e-96f5-40fc-8274-c486e400c5d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Seting up the Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c2fc8-b04e-4789-8291-90e6d5463521",
   "metadata": {},
   "source": [
    "## 4.1 Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24f2e673-189f-4acb-b2cf-1e1eec8ab51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatModel.from_pretrained(\"chat-bison@002\")\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6528edc0-de9d-4af3-89b9-7f9f809cc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Refer to the following context to answer this query: {query}\\n\\nContext: {context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ffcd6-f33c-4bd6-a20f-51734ee27b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  Who lives in america?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:27:32.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mretrieve\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mRetreived 1 chunk for the given embedding\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : John lives in America.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  Who lives in India?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:27:47.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mretrieve\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mRetreived 1 chunk for the given embedding\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : Aakansha lives in India.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  Does Aakansha have kids?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:28:07.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mretrieve\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mRetreived 1 chunk for the given embedding\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : The provided context does not mention whether Aakansha has kids or not.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  What can you tell me about Aakansha?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:28:25.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mretrieve\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mRetreived 1 chunk for the given embedding\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : Aakansha lives in India.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  Anything else?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-15 02:28:36.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mretrieve\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mRetreived 1 chunk for the given embedding\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : The provided context does not mention anything else.\n"
     ]
    }
   ],
   "source": [
    "chat = chat_model.start_chat(\n",
    "    context=\"\"\"\"\"\",\n",
    ")\n",
    "while True:\n",
    "    query = input(\"User Query: \")\n",
    "    if query == \"quit\":\n",
    "        break    \n",
    "    similar_chunks : list[str] = datastore.retrieve(query)\n",
    "    context : str = '\\n'.join(similar_chunks)\n",
    "    model_input = template.format(query = query, context = context)\n",
    "    response = chat.send_message(model_input)\n",
    "    print(f\"Model : {response.text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1dd53-eba2-4652-b31c-12446ebc6985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
