{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2416b63-f1bb-4f1d-9aad-6910260dac99",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "*DocuBot is a specialized chatbot designed for efficiently retrieving case-specific information from a collection of documents. By leveraging a pre-trained **large language model**, users can query DocuBot using natural language and receive relevant information from the document set.*\n",
    "\n",
    "*Behind the scenes, DocuBot generates **word embeddings** for input documents, organizes and stores them in a **PostgreSQL database**. When a user submits a query, DocuBot searches the database to contextually relevant responses.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ea259-e743-4d64-9348-dd683739905f",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03d7b1c-70bf-4bfb-987e-d09ef1f231bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pgvector\n",
    "import vertexai\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from psycopg2 import pool\n",
    "from loguru import logger\n",
    "from itertools import chain\n",
    "from pydantic import BaseModel\n",
    "from psycopg2.extras import execute_values\n",
    "from llama_index.core.schema import Document\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from vertexai.language_models import ChatModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from llama_index.core.text_splitter import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e71f3-f3c5-48d1-bfd1-e7cbe42d771b",
   "metadata": {},
   "source": [
    "# 1. Document Ingestion\n",
    "   \n",
    "- Preprocesses raw input, splitting it into logical segments.\n",
    "- Generates text embeddings for each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0716e2fc-7785-4417-894c-96b045fa5eb0",
   "metadata": {},
   "source": [
    "## 1.1 Split Text to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a6b79c-a1e0-4da8-a5b0-2fb3d4aecbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_to_chunks(input_text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a sentence into chunks\n",
    "    Input:\n",
    "        text : Text to be split\n",
    "    Output:\n",
    "        chunks: Segments of text after splitting\n",
    "    \"\"\"\n",
    "\n",
    "    # Parsing text with a preference for complete sentences\n",
    "    text_splitter = SentenceSplitter(\n",
    "        separator = \" \",\n",
    "        chunk_size = 300,\n",
    "        chunk_overlap = 20,\n",
    "        paragraph_separator = \"\\n\\n\",\n",
    "        secondary_chunking_regex = \"[^,.;。]+[,.;。]?\",\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode,\n",
    "    )\n",
    "    \n",
    "    txt_doc = Document(text = input_text)    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter([txt_doc])\n",
    "\n",
    "    return [chunk.text for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c0afd-4917-4a3b-a117-3ad5d70d1573",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 Generate Text Embeddings\n",
    "\n",
    "\\- Using a pre-trained model by Vertex AI to generate embeddings for input chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032e4389-5cbd-4feb-a28c-d43236df7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(BaseModel):\n",
    "    text : str\n",
    "    embedding : list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc95ae9-cc36-4316-82c6-da03229c6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text) -> list[float]:    \n",
    "    \"\"\"\n",
    "    Generate embeddings for given text\n",
    "    Input:\n",
    "        text : Input text   \n",
    "    Output:\n",
    "        vector: Emdedding of the input text\n",
    "    \"\"\"\n",
    "    \n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    embeddings = model.get_embeddings([text])\n",
    "    \n",
    "    for embedding in embeddings:\n",
    "        vector = embedding.values             \n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9941c1-cfe4-4c70-8f0a-aedc3703e9f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.3 Generate text, embedding pair for all chunks in a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb64241-46ce-403a-882a-896370f0cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding_pairs(text : str) -> list[TextEmbedding]:\n",
    "    \"\"\"\n",
    "    Get all the chunks and corresponding embeddings for a given text\n",
    "    Input:\n",
    "        text : Text whose chunk and embedding is needed\n",
    "    Output:\n",
    "        chunk_embedding_pairs: chunk and embedding of given text\n",
    "    \"\"\"    \n",
    "    \n",
    "    chunks : list[str] = split_input_to_chunks(text) \n",
    "    chunk_embedding_pairs : list[TextEmbedding] = []\n",
    "    logger.info(f'Number of chunks generated: {len(chunks)}')\n",
    "    \n",
    "    for curr_chunk in chunks:        \n",
    "        curr_embedding = text_embedding(curr_chunk)\n",
    "        chunk_embedding_pairs.append(TextEmbedding(text = curr_chunk, embedding = curr_embedding))    \n",
    "    \n",
    "    return chunk_embedding_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c8f04-3329-46a4-a6c9-fe43b0f2937c",
   "metadata": {},
   "source": [
    "# 2. Setting up the Cloud Database\n",
    "\n",
    "- Implementing functions to create and manage a database table for storing text chunks and their embeddings.\n",
    "- Utilizing connection pooling to efficiently manage database connections and reduce overhead.\n",
    "- Providing methods for inserting data into the table (ingest) and retrieving data from the table (retrieve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c4cea-4935-470e-9561-6c7cfdaf6be6",
   "metadata": {},
   "source": [
    "**Initializing Vertex AI environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48438211-986a-46c5-bd60-586348643718",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project = \"inductive-world-416413\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419283e3-6efb-429c-8a20-588c613a93a0",
   "metadata": {},
   "source": [
    "**Initializing DB parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3300dc-d909-498b-ab33-f2276f27e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PARAMS = {\n",
    "    'dbname' : \"vectordb\",\n",
    "    'user' : \"user\",\n",
    "    'password' : \"pwd\",\n",
    "    'host' : \"localhost\",\n",
    "    'port' : \"5432\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118353f-c38c-49c3-aa26-9818c58af6e1",
   "metadata": {},
   "source": [
    "**Setting up the Datastore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422b765d-b3f7-45ce-b4ad-0121e6e64f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStore:\n",
    "    \n",
    "    DATABASE_SCHEMA = {\n",
    "        \"text_chunk\" : \"varchar\",\n",
    "        \"embedding\" : \"vector(768)\"\n",
    "    }\n",
    "    \n",
    "    TABLE_NAME = \"my_table\"\n",
    "    \n",
    "    def __init__(self, db_params : dict = DB_PARAMS):\n",
    "        self.db_params = db_params        \n",
    "        self.conn_pool = self._get_connection_pool()        \n",
    "        self._create_table()\n",
    "    \n",
    "    def _get_connection_pool(self):\n",
    "        return psycopg2.pool.SimpleConnectionPool(1, 10, **self.db_params)\n",
    "\n",
    "    def _create_table(self) -> None:\n",
    "        col_defs = [f'{col_name} {col_type}' for col_name, col_type in self.DATABASE_SCHEMA.items()]        \n",
    "        cols = \", \".join(col_defs)        \n",
    "        table_creation_query = f\"\"\"\n",
    "            CREATE EXTENSION IF NOT EXISTS vector;\n",
    "            DROP TABLE IF EXISTS {self.TABLE_NAME};\n",
    "            CREATE TABLE IF NOT EXISTS {self.TABLE_NAME} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            {cols}\n",
    "            );\n",
    "            \"\"\"   \n",
    "        logger.info(table_creation_query)\n",
    "        try:\n",
    "            connection = self.conn_pool.getconn()\n",
    "            with connection:\n",
    "                with connection.cursor() as cursor:\n",
    "                    cursor.execute(table_creation_query)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in create table query: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.conn_pool.putconn(connection)\n",
    "    \n",
    "    def ingest(self, text: str) -> None:\n",
    "        text_embedding_pairs : list[TextChunk] = get_text_embedding_pairs(text)\n",
    "        data_list = [(curr.text, curr.embedding) for curr in text_embedding_pairs]\n",
    "        col_names = \",\".join(list(self.DATABASE_SCHEMA.keys()))\n",
    "        table_update_query = f\"\"\"\n",
    "            INSERT INTO {self.TABLE_NAME} \n",
    "            ( {col_names} )\n",
    "            VALUES %s\n",
    "            \"\"\"                    \n",
    "        try:            \n",
    "            connection = self.conn_pool.getconn()\n",
    "            with connection:\n",
    "                with connection.cursor() as cursor:\n",
    "                    execute_values(cursor, table_update_query, data_list)\n",
    "                    logger.info(\"Updated table with embedding pairs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in update table query : {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.conn_pool.putconn(connection)\n",
    "\n",
    "    def retrieve(self, query: str) -> str:\n",
    "        query_embedding : list[float] = text_embedding(query)\n",
    "        retrieval_query = f\"\"\"\n",
    "            SELECT text_chunk FROM {self.TABLE_NAME}\n",
    "            ORDER BY embedding <-> %s LIMIT 5\n",
    "            \"\"\"\n",
    "        retrieved_chunk = \"\"\n",
    "        try:            \n",
    "            connection = self.conn_pool.getconn()\n",
    "            register_vector(connection)\n",
    "            with connection:\n",
    "                with connection.cursor() as cursor:\n",
    "                    cursor.execute(retrieval_query, (np.array(query_embedding, dtype = np.float64), ))\n",
    "                    retrieved_chunk = list(chain.from_iterable(cursor.fetchall()))\n",
    "                    #logger.info(f\"Retreived {len(retrieved_chunk)} chunk for the given embedding\")            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in retrieval query : {e}\")\n",
    "            raise  \n",
    "        finally:\n",
    "            self.conn_pool.putconn(connection)\n",
    "        return retrieved_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfdd686-803f-4823-9550-838fee06bc20",
   "metadata": {},
   "source": [
    "# 3. Ingest Documents to Database\n",
    "\n",
    "\\- Reading text from input pdf and ingesting it into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1efeca40-1250-4a0f-83fc-593b61f287c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-17 18:44:02.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_create_table\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m\n",
      "            CREATE EXTENSION IF NOT EXISTS vector;\n",
      "            DROP TABLE IF EXISTS my_table;\n",
      "            CREATE TABLE IF NOT EXISTS my_table (\n",
      "            id SERIAL PRIMARY KEY,\n",
      "            text_chunk varchar, embedding vector(768)\n",
      "            );\n",
      "            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datastore = DataStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a118b8-b7c8-498e-bc2d-9a19ade92d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def read_pdf(file_path):    \n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:            \n",
    "            text += page.extract_text()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e226818e-9b92-48fc-9bf0-7e78deb96c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"/Users/aakanshadalmia/abc.pdf\"\n",
    "text = read_pdf(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde9ad42-b2dc-4395-b79b-e45999f2dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-17 18:44:03.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_text_embedding_pairs\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mNumber of chunks generated: 57\u001b[0m\n",
      "\u001b[32m2024-03-17 18:47:33.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mUpdated table with embedding pairs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_ingested = datastore.ingest(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc986e-96f5-40fc-8274-c486e400c5d0",
   "metadata": {},
   "source": [
    "# 4. Seting up the Chatbot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c2fc8-b04e-4789-8291-90e6d5463521",
   "metadata": {},
   "source": [
    "## 4.1 Initialize LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f2e673-189f-4acb-b2cf-1e1eec8ab51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatModel.from_pretrained(\"chat-bison@002\")\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dcc1b0-ed5a-4d84-a5cd-745052bb53be",
   "metadata": {},
   "source": [
    "## 4.2 Set up Model Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba4b9ea-db4f-40c7-9c58-61067fcbb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Refer to the following context to answer this query: {query}\\n\\nContext: {context}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b6bb4-51c9-4a87-8000-bae92dcf64ff",
   "metadata": {},
   "source": [
    "## 4.3 Set up Chat\n",
    "\n",
    "- Use prompt template to send user query as input to the chatbot\n",
    "- Store the chat history and update context to include this chat history\n",
    "- Use updated context to send another input to chatbot and use this answer as final responser sent to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc831975-d1ef-41ce-bcd9-46fd33f95749",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410ad969-9b6a-4433-aa58-31167885ff0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  What is geographical erasure?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : **Geographical erasure:** \n",
      "\n",
      "Geographical erasure refers to the phenomenon where language models underpredict or overlook certain geographical regions or countries in their generated text. This can occur when the training data used to develop the language model is biased towards certain regions or when the model lacks sufficient information about underrepresented areas.\n",
      "\n",
      "In the context of the provided text, geographical erasure is studied in the context of large language models (LLMs) and their tendency to capture information about dominant groups disproportionately. The paper investigates instances where LLMs underpredict the likelihood of certain countries appearing in generated text, despite those countries having significant English-speaking populations.\n",
      "\n",
      "The text highlights the importance of addressing geographical erasure to ensure that language models provide a more balanced and inclusive representation of the world.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  Describe fairness measures for language generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : **Fairness measures for language generation:** \n",
      "\n",
      "Fairness measures for language generation typically focus on identifying and mitigating biases in the generated text. These measures aim to ensure that the language models produce fair and unbiased outputs, free from discriminatory or harmful content.\n",
      "\n",
      "Common fairness measures for language generation include:\n",
      "\n",
      "1. **Demographic parity:** This measure assesses whether the generated text exhibits equal representation of different demographic groups, such as gender, race, or ethnicity. It ensures that the model does not favor or disfavor certain groups in its output.\n",
      "\n",
      "2. **Equality of opportunity:** This measure evaluates whether the generated text provides equal opportunities for different demographic groups. It ensures that the model does not generate text that reinforces existing societal biases or stereotypes.\n",
      "\n",
      "3. **Counterfactual fairness:** This measure assesses whether the generated text would remain fair if certain attributes of the input were changed. It helps identify and mitigate biases that may arise due to specific input features.\n",
      "\n",
      "4. **Individual fairness:** This measure focuses on ensuring that the generated text is fair to each individual, regardless of their group membership. It aims to prevent the model from making unfair predictions or generalizations about individuals based on their group affiliation.\n",
      "\n",
      "These fairness measures are essential for developing responsible and ethical language generation systems that produce inclusive and unbiased content.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  What was the goal of this paper?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : **Goal of the paper:** \n",
      "\n",
      "The goal of the paper is to study and operationalize a form of geographical erasure in language models, where these models underpredict the likelihood of certain countries appearing in generated text. The paper aims to demonstrate the existence of geographical erasure across different language models and investigate its causes and potential mitigation strategies.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  What was the method used to do this?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : **Method used:** \n",
      "\n",
      "The paper employs the following methods to study and mitigate geographical erasure in language models:\n",
      "\n",
      "1. **Erasure Measurement:** The paper defines a metric called \"Erasure\" (ER) to quantify the extent to which a language model underpredicts the likelihood of certain countries appearing in generated text. ER is calculated by comparing the model's predicted probabilities of countries with a ground truth distribution based on real-world population data.\n",
      "\n",
      "2. **Prompt Rephrasing:** To obtain a more comprehensive understanding of the model's world knowledge, the paper uses a set of diverse prompt wordings that encode the meaning of \"home country.\" These prompts are generated by paraphrasing a seed prompt using techniques such as prompting a language model (e.g., ChatGPT) and replacing sentence subjects.\n",
      "\n",
      "3. **Mitigation Strategy:** The paper proposes a mitigation strategy to alleviate geographical erasure by employing supervised fine-tuning. This involves fine-tuning the language model on a dataset specifically designed to address the underprediction of certain countries. The fine-tuning process aims to adjust the model's predictions to better align with the actual population distribution of countries.\n",
      "\n",
      "4. **Evaluation:** The effectiveness of the mitigation strategy is evaluated by measuring its impact on the perplexity of generated text on a standard language modeling benchmark (Wikitext-2-v1). Perplexity is a measure of how well the language model predicts the next word in a sequence of text.\n",
      "\n",
      "5. **Analysis:** The paper analyzes the causes of geographical erasure by examining factors such as training data bias, lack of diversity in training data, model size, and sampling strategies.\n",
      "\n",
      "These methods enable the paper to investigate the extent of geographical erasure in language models, identify its causes, and propose a practical mitigation strategy to address this issue.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  What conclusion was arrived at?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : **Conclusion:** \n",
      "\n",
      "The paper concludes that geographical erasure is a significant issue in large language models, leading to the underprediction of certain countries in generated text. The paper's analysis reveals that factors such as training data bias, lack of diversity in training data, model size, and sampling strategies contribute to geographical erasure.\n",
      "\n",
      "The paper proposes a mitigation strategy based on supervised fine-tuning to address geographical erasure. The fine-tuning process aims to adjust the model's predictions to better align with the actual population distribution of countries. The evaluation results show that the fine-tuning strategy effectively reduces geographical erasure while improving the model's performance on a standard language modeling benchmark.\n",
      "\n",
      "The paper highlights the importance of addressing geographical erasure to ensure fairness and inclusiveness in language generation systems. It\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User Query:  quit\n"
     ]
    }
   ],
   "source": [
    "chat = chat_model.start_chat(\n",
    "    context=\"\"\"\"\"\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    query = input(\"User Query: \")\n",
    "    if query == \"quit\":\n",
    "        break    \n",
    "        \n",
    "    # First API call to send the chat history and get the updated context\n",
    "    model_input_1 = prompt_template.format(query=\"\", context=\"\\n\".join(chat_history))\n",
    "    response_1 = chat.send_message(model_input_1)\n",
    "    updated_context = response_1.text.strip()\n",
    " \n",
    "    # Second API call to send the query with the updated context            \n",
    "    similar_chunks : list[str] = datastore.retrieve(query)\n",
    "    updated_context += '\\n'.join(similar_chunks)\n",
    "    model_input_2 = prompt_template.format(query = query, context = updated_context)\n",
    "    response_2 = chat.send_message(model_input_2)\n",
    "    \n",
    "    print(f\"Model : {response_2.text.strip()}\\n\\n\")\n",
    "\n",
    "    chat_history.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ff84a-9513-4f89-b9be-2ea6ee48b245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
